{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Fare Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compared to last few decades the passengers travelling via Air mode has increased drastically. Every other airline passenger tries to buy an air ticket at cheapest fare possible.__\n",
    "\n",
    "&ensp;\n",
    "\n",
    "<div>\n",
    "<img src = https://img.washingtonpost.com/rf/image_1484w/2010-2019/WashingtonPost/2017/06/29/Interactivity/Images/iStock-626867464.JPG?uuid=zUBRYlq-EeeqaTlkp9VSBw width=\"800\">\n",
    "<div/>\n",
    " <center> Flight Fares </center>\n",
    "\n",
    "&ensp;\n",
    "\n",
    "\n",
    " To achieve this we have to follow one basic rule that is plan your travel well in advance, but it doesn't always guarantee you end up buying cheapest air fare ticket. And always its not possible to plan our travel in advance. Flight fare in today's world is difficult to predict as it keep varying on frequent basis. \n",
    " \n",
    "As a Data Scientist, here is the attempt to predict the air fares for various airlines in India."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.2.0-py3-none-win_amd64.whl (86.5 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\tushar_dalal\\anaconda3\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\tushar_dalal\\anaconda3\\lib\\site-packages (from xgboost) (1.5.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1541d8cda27c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElasticNetCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "## Import library\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings # To supress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np # Linear algebraa\n",
    "import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # Matlab-style plottinga\n",
    "import seaborn as sns # Visualisation\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We have 2 datasets 'Train' and 'Test', both consists of categorical and numerical variables. 'Train' and 'Test' both contain similar columns except there is no 'Price' column 'Test' dataset as same has to be predicted. Let us load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load datasets\n",
    "\n",
    "train_df =  pd.read_excel('Data_Train.xlsx')\n",
    "test_df=pd.read_excel('Test_set.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining datasets into one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine test dataset and train dataset so as to work on both datasets at same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = train_df.append(test_df)\n",
    "combined_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling, EDA and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Price\" column is __Target Variable__ which has to be predicted in test dataset. The other variables are features.\n",
    "\n",
    "Additional_Info : Info about type of meal or any other service passenger is willing to opt.\n",
    "\n",
    "- Airline: __Name of the airline__\n",
    "- Arrival_Time      \n",
    "- Date_of_Journey  \n",
    "- Dep_Time:__Time of Departure___\n",
    "- Destination\n",
    "- Source  \n",
    "- Duration: __Total duration of the flight___\n",
    "- Route: __Flight will travel via these cities__\n",
    "- Total_Stops:__Total stops flight will have in the journey___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We can see only 'Index' and 'Price' columns are in numeric format._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Date Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date of Journey column is in dd/mm/yyyy format but its datatype is object. We need to convert this column into datetime datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Date_of_Journey'] =  pd.to_datetime(combined_df['Date_of_Journey'],format ='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting Date \n",
    "\n",
    "combined_df['Date'] = combined_df['Date_of_Journey'].dt.day.astype(int)\n",
    "combined_df['Month'] = combined_df['Date_of_Journey'].dt.month.astype(int)\n",
    "combined_df['Year'] = combined_df['Date_of_Journey'].dt.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As we have extracted Date, Month & Year from 'Date_of_Journey' column, we can drop this column.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(['Date_of_Journey'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace NA values in Price column with mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Price'].fillna((combined_df['Price'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot \n",
    "\n",
    "Let's try to do some analytics by plotting a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"Date\", y=\"Price\" ,kind = 'reg', data=combined_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot describes that Ticket fares are slightly cheaper in mid of the month as compared to start and end of the month. Majority of tickets are of the range Rs. 1700 to Rs.18000. There is light negative corelation between Ticket Price and Date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Arrival_Time & Dep_Time Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In 'Arrival_Time' column, the time is in combined format of Date & time but we don't need date from it so we will strip date and extract only time from it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Arrival_Time'] = combined_df['Arrival_Time'] .str.split(' ').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting 'Hour' & \"Minutes' in separate columns from \"Arrival_Time\" and \"Dep_Time\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Arrival_Hour'] = combined_df['Arrival_Time'] .str.split(':').str[0].astype(int)\n",
    "combined_df['Arrival_Minute'] = combined_df['Arrival_Time'] .str.split(':').str[1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df.drop(['Arrival_Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Dep_Hour'] = combined_df['Dep_Time'] .str.split(':').str[0].astype(int)\n",
    "combined_df['Dep_Minute'] = combined_df['Dep_Time'] .str.split(':').str[1].astype(int)\n",
    "combined_df=combined_df.drop(['Dep_Time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing 'Total Stop' Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__'Total Stop' column contains values such as '2 Stop', '1 Stop', 'non-stop', we will replace 'non stop' with '0 stop' and get only integers out of values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Total_Stops'] = combined_df['Total_Stops'].fillna('1 stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Total_Stops']=combined_df['Total_Stops'].replace('non-stop','0 stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Stop'] = combined_df['Total_Stops'].str.split(' ').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Total_Stops' can be dropped as we have extracted numeric values in 'Stop' \n",
    "\n",
    "combined_df=combined_df.drop(['Total_Stops'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Route Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to eliminate \"→\" symbol from 'Route' column and extract city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Route_1'] = combined_df['Route'] .str.split('→ ').str[0]\n",
    "combined_df['Route_2'] = combined_df['Route'] .str.split('→ ').str[1]\n",
    "combined_df['Route_3'] = combined_df['Route'] .str.split('→ ').str[2]\n",
    "combined_df['Route_4'] = combined_df['Route'] .str.split('→ ').str[3]\n",
    "combined_df['Route_5'] = combined_df['Route'] .str.split('→ ').str[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace NA values in 'Route_n' columns with mean None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Route_1'].fillna(\"None\",inplace = True)\n",
    "combined_df['Route_2'].fillna(\"None\",inplace = True)\n",
    "combined_df['Route_3'].fillna(\"None\",inplace = True)\n",
    "combined_df['Route_4'].fillna(\"None\",inplace = True)\n",
    "combined_df['Route_5'].fillna(\"None\",inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer Encoding\n",
    "\n",
    "We will encode categorical data in our dataset to numerical data using Label Encoder.\n",
    "\n",
    "For this we will import LabelEncoder from sklearn library,then fit and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_encode = LabelEncoder()\n",
    "combined_df[\"Additional_Info\"] = lb_encode.fit_transform(combined_df[\"Additional_Info\"])\n",
    "combined_df[\"Airline\"] = lb_encode.fit_transform(combined_df[\"Airline\"])\n",
    "combined_df[\"Destination\"] = lb_encode.fit_transform(combined_df[\"Destination\"])\n",
    "combined_df[\"Source\"] = lb_encode.fit_transform(combined_df[\"Source\"])\n",
    "combined_df['Route_1']= lb_encode.fit_transform(combined_df[\"Route_1\"])\n",
    "combined_df['Route_2']= lb_encode.fit_transform(combined_df[\"Route_2\"])\n",
    "combined_df['Route_3']= lb_encode.fit_transform(combined_df[\"Route_3\"])\n",
    "combined_df['Route_4']= lb_encode.fit_transform(combined_df[\"Route_4\"])\n",
    "combined_df['Route_5']= lb_encode.fit_transform(combined_df[\"Route_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding - Dummy encoding\n",
    "\n",
    "In addition to 'Integer Encoding', we will apply 'Dummy Encoding' to disallow our model to assume any natural ordering between categorie as this may result in poor performance. \n",
    "\n",
    "This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
    "\n",
    "We will apply get_dummies class from Pandas library to each column and then drop original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Additional_Info_dummies = pd.get_dummies(combined_df[\"Additional_Info\"], prefix='Additional_Info')    \n",
    "combined_df = pd.concat([combined_df, Additional_Info_dummies], axis=1)\n",
    "combined_df.drop('Additional_Info', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dummies = pd.get_dummies(combined_df[\"Airline\"], prefix='Airline')    \n",
    "combined_df = pd.concat([combined_df, Airline_dummies], axis=1)\n",
    "combined_df.drop('Airline', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Destination_dummies = pd.get_dummies(combined_df[\"Destination\"], prefix='Destination')    \n",
    "combined_df = pd.concat([combined_df, Destination_dummies], axis=1)\n",
    "combined_df.drop('Destination', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_dummies = pd.get_dummies(combined_df[\"Source\"], prefix='Source')    \n",
    "combined_df = pd.concat([combined_df, Source_dummies], axis=1)\n",
    "combined_df.drop('Source', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Route_1_dummies = pd.get_dummies(combined_df[\"Route_1\"], prefix='Route_1')    \n",
    "combined_df = pd.concat([combined_df, Route_1_dummies], axis=1)\n",
    "combined_df.drop('Route_1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Route_2_dummies = pd.get_dummies(combined_df[\"Route_2\"], prefix='Route_2')    \n",
    "combined_df = pd.concat([combined_df, Route_2_dummies], axis=1)\n",
    "combined_df.drop('Route_2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Route_3_dummies = pd.get_dummies(combined_df[\"Route_3\"], prefix='Route_3')    \n",
    "combined_df = pd.concat([combined_df, Route_3_dummies], axis=1)\n",
    "combined_df.drop('Route_3', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Route_4_dummies = pd.get_dummies(combined_df[\"Route_4\"], prefix='Route_4')    \n",
    "combined_df = pd.concat([combined_df, Route_4_dummies], axis=1)\n",
    "combined_df.drop('Route_4', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Route_5_dummies = pd.get_dummies(combined_df[\"Route_5\"], prefix='Route_5')    \n",
    "combined_df = pd.concat([combined_df, Route_5_dummies], axis=1)\n",
    "combined_df.drop('Route_5', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df.drop(['Route'], axis=1)\n",
    "combined_df=combined_df.drop(['Duration'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST TRAIN SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate out train set and test set from the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it into test and train\n",
    "\n",
    "df_train = combined_df[0:10683]\n",
    "df_test = combined_df[10683:]\n",
    "df_test = df_test.drop(['Price'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['Price'], axis=1)\n",
    "y = df_train.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different models and compare the \"RMSE\" score for each model.\n",
    "\n",
    "We are going to try below ML algorithms :\n",
    "- LinearRegression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Elastic Net Regularization\n",
    "- Extreme Gradient Boosting (XGBoost)\n",
    "- Light GBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression() #LinearRegression\n",
    "rig_cv = RidgeCV() #Ridge Regression\n",
    "lasso = LassoCV() #Lasso Regression\n",
    "elastic = ElasticNetCV() #Elastic Net Regularization\n",
    "xgb = XGBRegressor() #Extreme Gradient Boosting (XGBoost)\n",
    "lig_gbm = LGBMRegressor() #Light GBM\n",
    "\n",
    "models = [lin_reg, rig_cv, lasso, elastic, xgb, lig_gbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our cross validation method\n",
    "kfolds = KFold(n_splits=50,shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_rmse(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, \n",
    "                                   scoring=\"neg_mean_squared_error\", \n",
    "                                   cv = kfolds))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "acc = []\n",
    "for model in models:\n",
    "    print ('Cross-validation of : {0}'.format(model.__class__))\n",
    "    score = cv_rmse(model).mean()\n",
    "    acc.append(score)\n",
    "    print ('CV score = {0}'.format(score))\n",
    "    print ('****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, after applying different Regression models we can see XGboost is performing really good as compared to others.\n",
    "\n",
    "So we will use __'XGboost'__ to predict our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train,y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply above model to predict \"Price\" for original test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_xgb = df_test\n",
    "xgb_pred = xgb.predict(df_test)\n",
    "df_test_xgb['Price'] = xgb_pred\n",
    "df_test_xgb.to_csv('flight_price_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such work around, Feature Engineering plays an important role. Also here we have used dual encoding techniques to increase our model's performance.\n",
    "\n",
    "We compared RMSE score for each model and then selected the model with better RMSE score to apply on our test dataset.\n",
    "\n",
    "Advanced techniques like Pipeline, Stacking etc. can be used to tune algorithm and improve the performance of the model.\n",
    "\n",
    "Further, Hyperparameter tuning can be used to fine tune our alogorithm and get best performance score from the model.\n",
    "\n",
    "Source :\n",
    "\n",
    "https://medium.com/code-to-express/flight-price-prediction-7c83616a13bb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(‘fppmodel.pkl’, ‘wb’))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
